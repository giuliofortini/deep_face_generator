{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "face_generator.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "X1eJXjC81Voj",
        "m5Nq_6sJ11mp",
        "seD1MXCi2FFR"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_te4vo61RQ8"
      },
      "source": [
        "## Imports and drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHJh32AXxZ-p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a35e09f5-88ea-41cf-a278-e3af8dac6d5c"
      },
      "source": [
        "from keras import Model\n",
        "from keras import backend\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras.initializers import RandomNormal\n",
        "from keras.constraints import Constraint\n",
        "from keras.layers import Dense, Input, Conv2D, Concatenate, Flatten, BatchNormalization, Embedding, Activation, Dropout, LeakyReLU, ReLU, Reshape, Conv2DTranspose, concatenate, MaxPooling2D\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.preprocessing.image import ImageDataGenerator, load_img\n",
        "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import numpy as np\n",
        "import os, time, random, zipfile, cv2\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import files\n",
        "from urllib import request\n",
        "from google.colab import drive\n",
        "from scipy.linalg import sqrtm\n",
        "from PIL import Image, ImageOps\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eGb1I6VIIk5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65d54621-7451-4228-8258-4b0675aec095"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDRcDYmHe1OZ"
      },
      "source": [
        "## Paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjhgzyTse0iA"
      },
      "source": [
        "dataset_folder = os.path.join(os.getcwd(), \"Dataset\")\r\n",
        "dataset_path = os.path.join(dataset_folder, \"celeba.zip\")\r\n",
        "extract_path = os.path.join(dataset_folder, \"img_align_celeba\")\r\n",
        "cropped_path = os.path.join(dataset_folder, \"Cropped\")\r\n",
        "attributes_path = os.path.join(dataset_folder, \"list_attr_celeba.csv\")\r\n",
        "generated_images_path = os.path.join(os.getcwd(), \"Generated images\")\r\n",
        "generator_path = os.path.join(os.getcwd(), \"Generators\")\r\n",
        "discriminator_path = os.path.join(os.getcwd(), \"Discriminators\")\r\n",
        "gan_path = os.path.join(os.getcwd(), \"GAN\")\r\n",
        "g_model_path = os.path.join(generator_path, \"generator.h5\")\r\n",
        "g_model_path_drive = os.path.join(\"/content/drive/MyDrive/Generators\", \"generator.h5\")\r\n",
        "d_model_path = os.path.join(discriminator_path, \"discriminator.h5\")\r\n",
        "d_model_path_drive = os.path.join(\"/content/drive/MyDrive/Discriminators\", \"discriminator.h5\")\r\n",
        "gan_model_path = os.path.join(generator_path, \"gan.h5\")\r\n",
        "gan_model_path_drive = os.path.join(\"/content/drive/MyDrive/GAN\", \"gan.h5\")\r\n",
        "\r\n",
        "if not os.path.exists(generator_path):\r\n",
        "  os.makedirs(generator_path)\r\n",
        "\r\n",
        "if not os.path.exists(discriminator_path):\r\n",
        "  os.makedirs(discriminator_path)\r\n",
        "\r\n",
        "if not os.path.exists(gan_path):\r\n",
        "  os.makedirs(gan_path)\r\n",
        "\r\n",
        "if not os.path.exists(dataset_folder):\r\n",
        "    os.makedirs(dataset_folder)\r\n",
        "\r\n",
        "if not os.path.exists(cropped_path):\r\n",
        "    os.makedirs(cropped_path)\r\n",
        "\r\n",
        "if not os.path.exists(generated_images_path):\r\n",
        "    os.makedirs(generated_images_path)\r\n",
        "\r\n",
        "dataset_url =  \"https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/celeba.zip\""
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5w1in7AISM5"
      },
      "source": [
        "!cp '/content/drive/MyDrive/list_attr_celeba.csv' '/content/Dataset'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1eJXjC81Voj"
      },
      "source": [
        "## Dataset download and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wLwnC6Px9aT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ed47c91-4182-4a71-f4b1-b4162fbb1a57"
      },
      "source": [
        "def download_dataset(download_path, url):\n",
        "  if not os.path.exists(download_path):\n",
        "    print(\"Downloading dataset...\", end=\" \")\n",
        "    request.urlretrieve(url, download_path)\n",
        "    print(\"Done!\")\n",
        "  else:\n",
        "    print(\"Already downloaded\")\n",
        "\n",
        "def extract_dataset(download_path, dataset_folder, extract_path):\n",
        "  if not os.path.exists(extract_path):\n",
        "    print(\"Extracting dataset...\", end=\" \")\n",
        "    with zipfile.ZipFile(download_path, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(dataset_folder)\n",
        "    print(\"Done!\")\n",
        "  else:\n",
        "    print(\"Already extracted\")\n",
        "\n",
        "# Download\n",
        "download_dataset(dataset_path, dataset_url)\n",
        "\n",
        "# Extraction\n",
        "extract_dataset(dataset_path, dataset_folder, extract_path)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading dataset... Done!\n",
            "Extracting dataset... Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5cR4wMeN0QX"
      },
      "source": [
        "def crop(source_path, dest_path):\n",
        "  if not os.path.isdir(dest_path):\n",
        "    os.mkdir(dest_path)\n",
        "  if len(os.listdir(dest_path)) == 0:\n",
        "    images = os.listdir(source_path)\n",
        "    for img_path in images:\n",
        "      img = Image.open(source_path + \"/\" + img_path)\n",
        "      f, e = os.path.splitext(img_path)\n",
        "      imCrop = img.crop((25, 45, 128 + 25, 128 + 45)) \n",
        "      imCrop.save(dest_path + \"/\" + f + '.jpg', \"JPEG\", quality=100)\n",
        "\n",
        "crop(extract_path, cropped_path)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exKAdXSeXtWP"
      },
      "source": [
        "def load_attributes(path):\n",
        "  df = pd.read_csv(path)\n",
        "  ids = df['image_id'] \n",
        "  attributes = df.drop(columns=['image_id'])\n",
        "  attributes.replace(to_replace=-1, value=0, inplace=True)\n",
        "  attributes = attributes.apply(lambda x: x.to_numpy(), axis=1)\n",
        "  df = pd.DataFrame(columns = ['file_id', 'attributes'])\n",
        "  df['file_id'] = ids\n",
        "  df['attributes'] = attributes\n",
        "  return df\n",
        "  \n",
        "attributes = load_attributes(attributes_path)\n",
        "attributes_list = pd.read_csv(attributes_path).drop(columns=['image_id']).columns\n",
        "attributes_dict = dict(enumerate(attributes_list))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4d1Y2HoXvbs"
      },
      "source": [
        "def create_custom_label(attr):\r\n",
        "  label = []\r\n",
        "  for k,v in attributes_dict.items():\r\n",
        "    if v in attr:\r\n",
        "      label.append(1)\r\n",
        "    else:\r\n",
        "      label.append(0)\r\n",
        "  return np.array(label)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwU9PDQSLXJy"
      },
      "source": [
        "def decode_label(label):\r\n",
        "  attr = []\r\n",
        "  for i,element in enumerate(label):\r\n",
        "    if element == 1:\r\n",
        "      attr.append(attributes_dict[i].replace(\"'\", \"\"))\r\n",
        "  return ', '.join(attr)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uGPh3Z4h06u"
      },
      "source": [
        "def concat_images(images, size, shape=None):\n",
        "    # Open images and resize them\n",
        "    width, height = size\n",
        "  \n",
        "    # Create canvas for the final image with total size\n",
        "    shape = shape if shape else (1, len(images))\n",
        "    image_size = (width * shape[1], height * shape[0])\n",
        "    image = Image.new('RGB', image_size)\n",
        "    \n",
        "    # Paste images into final image\n",
        "    for row in range(shape[0]):\n",
        "        for col in range(shape[1]):\n",
        "            offset = width * col, height * row\n",
        "            idx = row * shape[1] + col\n",
        "            image.paste(images[idx], offset)\n",
        "    \n",
        "    return image\n",
        "\n",
        "# Get list of image paths\n",
        "image_paths = [os.path.join(cropped_path, f) \n",
        "               for f in os.listdir(cropped_path) if f.endswith('.jpg')]\n",
        "\n",
        "# Random selection of images\n",
        "image_array = random.choices(image_paths, k=84)\n",
        "\n",
        "images = map(Image.open, image_array)\n",
        "images = [ImageOps.fit(image, (64, 64), Image.ANTIALIAS) \n",
        "              for image in images]\n",
        "\n",
        "# Create and save image grid\n",
        "image = concat_images(images, (64, 64), (8, 8))\n",
        "plt.figure(figsize = (16,16))\n",
        "plt.imshow(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiGwDhz9ye_n"
      },
      "source": [
        "def load_dataset(cropped_path, attributes, batch_size, image_shape, subset=False):\n",
        "    dataset_generator = ImageDataGenerator()\n",
        "    if subset:\n",
        "      dataset_generator = dataset_generator.flow_from_dataframe(\n",
        "        dataframe=attributes[0:20000], directory=cropped_path, target_size=(image_shape[0], image_shape[1]),\n",
        "        batch_size=batch_size, x_col='file_id', y_col='attributes', class_mode='raw')\n",
        "    else:\n",
        "      dataset_generator = dataset_generator.flow_from_dataframe(\n",
        "        dataframe=attributes, directory=cropped_path, target_size=(image_shape[0], image_shape[1]),\n",
        "        batch_size=batch_size, x_col='file_id', y_col='attributes', class_mode='raw')\n",
        "\n",
        "    return dataset_generator"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKWYOTNi1cJa"
      },
      "source": [
        "## Gan section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5Nq_6sJ11mp"
      },
      "source": [
        "### Function to save generated images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8bIzPn29AGg"
      },
      "source": [
        "def save_generated_images(generated_images, epoch, batch_number, batch_size):\n",
        "\n",
        "  plt.figure(figsize=(8, 8), num=2)\n",
        "  gs1 = gridspec.GridSpec(8, 8)\n",
        "  gs1.update(wspace=0, hspace=0)\n",
        "\n",
        "  for i in range(batch_size):\n",
        "      ax1 = plt.subplot(gs1[i])\n",
        "      ax1.set_aspect('equal')\n",
        "      image = generated_images[i, :, :, :]\n",
        "      image += 1\n",
        "      image *= 127.5\n",
        "      fig = plt.imshow(image.astype(np.uint8))\n",
        "      plt.axis('off')\n",
        "      fig.axes.get_xaxis().set_visible(False)\n",
        "      fig.axes.get_yaxis().set_visible(False)\n",
        "      \n",
        "  plt.tight_layout()\n",
        "  save_name = 'Generated images/epoch_' + str(\n",
        "      epoch + 1) + '_batch_' + str(batch_number + 1) + '.png'\n",
        "\n",
        "  plt.savefig(save_name, bbox_inches='tight', pad_inches=0)\n",
        "  plt.pause(0.0000000001)\n",
        "  plt.show()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcUGzwo31iiR"
      },
      "source": [
        "### Wasserstein loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLAQDfLxSDZV"
      },
      "source": [
        "def wasserstein_loss(y_true, y_pred):\r\n",
        "\treturn backend.mean(y_true * y_pred)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_mO4peNHhL3"
      },
      "source": [
        "### Focal loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88e4jZpnHgQz"
      },
      "source": [
        "def binary_focal_loss(y_true, y_pred):\r\n",
        "  \"\"\"\r\n",
        "  :param y_true: A tensor of the same shape as `y_pred`\r\n",
        "  :param y_pred:  A tensor resulting from a sigmoid\r\n",
        "  :return: Output tensor.\r\n",
        "  \"\"\"\r\n",
        "  gamma=2.\r\n",
        "  alpha=.25\r\n",
        "  \r\n",
        "  y_true = tf.cast(y_true, tf.float32)\r\n",
        "  # Define epsilon so that the back-propagation will not result in NaN for 0 divisor case\r\n",
        "  epsilon = backend.epsilon()\r\n",
        "\r\n",
        "  # Clip the prediciton value\r\n",
        "  y_pred = backend.clip(y_pred, epsilon, 1.0 - epsilon)\r\n",
        "  \r\n",
        "  # Calculate p_t\r\n",
        "  p_t = tf.where(backend.equal(y_true, 1), y_pred, 1 - y_pred)\r\n",
        "  \r\n",
        "  # Calculate alpha_t\r\n",
        "  alpha_factor = backend.ones_like(y_true) * alpha\r\n",
        "  alpha_t = tf.where(backend.equal(y_true, 1), alpha_factor, 1 - alpha_factor)\r\n",
        "  \r\n",
        "  # Calculate cross entropy\r\n",
        "  cross_entropy = -backend.log(p_t)\r\n",
        "  weight = alpha_t * backend.pow((1 - p_t), gamma)\r\n",
        "  \r\n",
        "  # Calculate focal loss\r\n",
        "  loss = weight * cross_entropy\r\n",
        "  \r\n",
        "  # Sum the losses in mini_batch\r\n",
        "  loss = backend.mean(backend.sum(loss, axis=1))\r\n",
        "  \r\n",
        "  return loss"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seD1MXCi2FFR"
      },
      "source": [
        "### FID function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mv2RimHG_SjK"
      },
      "source": [
        "def scale_images(images, new_shape):\n",
        "  images_list = list()\n",
        "\n",
        "  for image in images:\n",
        "    # resize with bilinear interpolation\n",
        "\t  new_image = cv2.resize(image, (new_shape, new_shape))\n",
        "    # store\n",
        "\t  images_list.append(new_image)\n",
        "  return np.asarray(images_list)\n",
        "\n",
        "def calculate_fid(model, images1, images2):\n",
        "  # scale images\n",
        "  images1 = scale_images(images1, 299)\n",
        "  images2 = scale_images(images2, 299)\n",
        "\n",
        "  # calculate activations\n",
        "  act1 = model.predict(images1)\n",
        "  act2 = model.predict(images2)\n",
        "\n",
        "  # calculate mean and covariance statistics\n",
        "  mu1, sigma1 = act1.mean(axis=0), np.cov(act1, rowvar=False)\n",
        "  mu2, sigma2 = act2.mean(axis=0), np.cov(act2, rowvar=False)\n",
        "\n",
        "  # calculate sum squared difference between means\n",
        "  ssdiff = np.sum((mu1 - mu2)**2.0)\n",
        "\n",
        "  # calculate sqrt of product between cov\n",
        "  covmean = sqrtm(sigma1.dot(sigma2))\n",
        "\n",
        "  # check and correct imaginary numbers from sqrt\n",
        "  if np.iscomplexobj(covmean):\n",
        "    covmean = covmean.real\n",
        "\n",
        "  # calculate score\n",
        "  fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
        "  return fid"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6X3LdUd01scN"
      },
      "source": [
        "### Real and Fake data generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoNUjv5z1gjf"
      },
      "source": [
        "# generate points in latent space as input for the generator\n",
        "def generate_latent_points(latent_dim, n_samples):\n",
        "  # generate points in the latent space\n",
        "  x_input = np.random.randn(latent_dim * n_samples)\n",
        "  \n",
        "  # reshape into a batch of inputs for the network\n",
        "  x_input = x_input.reshape(n_samples, latent_dim)\n",
        "  \n",
        "  # generate labels\n",
        "  labels = np.array(random.sample(list(attributes['attributes']), n_samples))\n",
        "  \n",
        "  return [x_input, labels]\n",
        "\n",
        "def generate_real_samples(dataset):\n",
        "  X, label = dataset.next()\n",
        "  X /= 127.5\n",
        "  X -= 1\n",
        "  y = -np.ones(X.shape[0])\n",
        "  return [X, label.tolist()], y\n",
        "\n",
        "# use the generator to generate n fake examples, with class labels\n",
        "def generate_fake_samples(generator, latent_dim, n_samples, custom_label=None):\n",
        "  \n",
        "  # generate points in latent space\n",
        "  x_input, labels = generate_latent_points(latent_dim, n_samples)\n",
        "  \n",
        "  if custom_label is not None:\n",
        "    labels = np.tile(custom_label, (n_samples,1))\n",
        "  \n",
        "  # predict outputs\n",
        "  X = generator.predict([x_input, labels])\n",
        "  \n",
        "  # create class labels\n",
        "  y = np.ones((n_samples, 1))\n",
        "  \n",
        "  return [X, labels], y"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2wwwHS-_VMn"
      },
      "source": [
        "### Spectral Normalization and Clipping Constraint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOicbwjQzwD8"
      },
      "source": [
        "class SpectralNormalization(tf.keras.layers.Wrapper):\r\n",
        "    \"\"\"Performs spectral normalization on weights.\r\n",
        "    This wrapper controls the Lipschitz constant of the layer by\r\n",
        "    constraining its spectral norm, which can stabilize the training of GANs.\r\n",
        "    See [Spectral Normalization for Generative Adversarial Networks](https://arxiv.org/abs/1802.05957).\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, layer: tf.keras.layers, power_iterations: int = 1, **kwargs):\r\n",
        "        super().__init__(layer, **kwargs)\r\n",
        "        if power_iterations <= 0:\r\n",
        "            raise ValueError(\r\n",
        "                \"`power_iterations` should be greater than zero, got \"\r\n",
        "                \"`power_iterations={}`\".format(power_iterations)\r\n",
        "            )\r\n",
        "        self.power_iterations = power_iterations\r\n",
        "        self._initialized = False\r\n",
        "\r\n",
        "    def build(self, input_shape):\r\n",
        "        \"\"\"Build `Layer`\"\"\"\r\n",
        "        super().build(input_shape)\r\n",
        "        input_shape = tf.TensorShape(input_shape)\r\n",
        "        self.input_spec = tf.keras.layers.InputSpec(shape=[None] + input_shape[1:])\r\n",
        "\r\n",
        "        if hasattr(self.layer, \"kernel\"):\r\n",
        "            self.w = self.layer.kernel\r\n",
        "        elif hasattr(self.layer, \"embeddings\"):\r\n",
        "            self.w = self.layer.embeddings\r\n",
        "        else:\r\n",
        "            raise AttributeError(\r\n",
        "                \"{} object has no attribute 'kernel' nor \"\r\n",
        "                \"'embeddings'\".format(type(self.layer).__name__)\r\n",
        "            )\r\n",
        "\r\n",
        "        self.w_shape = self.w.shape.as_list()\r\n",
        "\r\n",
        "        self.u = self.add_weight(\r\n",
        "            shape=(1, self.w_shape[-1]),\r\n",
        "            initializer=tf.initializers.TruncatedNormal(stddev=0.02),\r\n",
        "            trainable=False,\r\n",
        "            name=\"sn_u\",\r\n",
        "            dtype=self.w.dtype,\r\n",
        "        )\r\n",
        "\r\n",
        "    def call(self, inputs, training=None):\r\n",
        "        \"\"\"Call `Layer`\"\"\"\r\n",
        "        if training is None:\r\n",
        "            training = tf.keras.backend.learning_phase()\r\n",
        "\r\n",
        "        if training:\r\n",
        "            self.normalize_weights()\r\n",
        "\r\n",
        "        output = self.layer(inputs)\r\n",
        "        return output\r\n",
        "\r\n",
        "    def compute_output_shape(self, input_shape):\r\n",
        "        return tf.TensorShape(self.layer.compute_output_shape(input_shape).as_list())\r\n",
        "\r\n",
        "    @tf.function\r\n",
        "    def normalize_weights(self):\r\n",
        "        \"\"\"Generate spectral normalized weights.\r\n",
        "        This method will update the value of `self.w` with the\r\n",
        "        spectral normalized value, so that the layer is ready for `call()`.\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        w = tf.reshape(self.w, [-1, self.w_shape[-1]])\r\n",
        "        u = self.u\r\n",
        "\r\n",
        "        with tf.name_scope(\"spectral_normalize\"):\r\n",
        "            for _ in range(self.power_iterations):\r\n",
        "                v = tf.math.l2_normalize(tf.matmul(u, w, transpose_b=True))\r\n",
        "                u = tf.math.l2_normalize(tf.matmul(v, w))\r\n",
        "\r\n",
        "            sigma = tf.matmul(tf.matmul(v, w), u, transpose_b=True)\r\n",
        "\r\n",
        "            self.w.assign(self.w / sigma)\r\n",
        "            self.u.assign(u)\r\n",
        "\r\n",
        "    def get_config(self):\r\n",
        "        config = {\"power_iterations\": self.power_iterations}\r\n",
        "        base_config = super().get_config()\r\n",
        "        return {**base_config, **config}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j34v48IgzxAb"
      },
      "source": [
        "class ClipConstraint(Constraint):\r\n",
        "  # set clip value when initialized\r\n",
        "  def __init__(self, clip_value):\r\n",
        "    self.clip_value = clip_value\r\n",
        "\r\n",
        "  # clip model weights to hypercube\r\n",
        "  def __call__(self, weights):\r\n",
        "    return backend.clip(weights, -self.clip_value, self.clip_value)\r\n",
        "\r\n",
        "  # get the config\r\n",
        "  def get_config(self):\r\n",
        "    return {'clip_value': self.clip_value}\r\n",
        "\r\n",
        "const = ClipConstraint(0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMKzfqfA_Mzt"
      },
      "source": [
        "### Discriminator\r\n",
        "\r\n",
        "Tried to remove kernel clip constraint and add Spectral Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BW_hIS3w1Jtk",
        "cellView": "code"
      },
      "source": [
        "def define_discriminator(in_shape=(64,64,3), label_shape=(40,), n_attributes=40):\n",
        "\n",
        "  init = RandomNormal(stddev=0.02)\n",
        "\n",
        "  input_image = Input(shape=in_shape)\n",
        "\n",
        "  # define the constraint\n",
        "  const = ClipConstraint(0.01)\n",
        "\n",
        "\t# downsample\n",
        "  d = SpectralNormalization(Conv2D(32, (4,4), strides=(2,2), padding='same', input_shape=in_shape, kernel_initializer=init))(input_image)\n",
        "  d = LeakyReLU(alpha=0.2)(d)\n",
        "  d = Dropout(0.2)(d)\n",
        "\t\n",
        "  # downsample\n",
        "  d = SpectralNormalization(Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init))(d)\n",
        "  d = LeakyReLU(alpha=0.2)(d)\n",
        "  d = Dropout(0.2)(d)\n",
        "  \n",
        "  # downsample\n",
        "  d = SpectralNormalization(Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init))(d)\n",
        "  d = LeakyReLU(alpha=0.2)(d)\n",
        "  d = Dropout(0.2)(d)\n",
        "  \n",
        "  # downsample\n",
        "  d = SpectralNormalization(Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init))(d)\n",
        "  d = LeakyReLU(alpha=0.2)(d)\n",
        "  d = Dropout(0.2)(d)\n",
        "  \n",
        "  # classifier\n",
        "  d = Flatten()(d)\n",
        "  d = Dropout(0.4)(d)\n",
        "  \n",
        "  out1 = Dense(1, activation='linear')(d)\n",
        "  out2 = Dense(n_attributes, activation='sigmoid')(d)\n",
        "\t\n",
        "  # compile model\n",
        "  opt = RMSprop(lr=0.00005)\n",
        "\n",
        "  model = Model(input_image,  [out1, out2])\n",
        "  \n",
        "  model.compile(loss=[wasserstein_loss, binary_focal_loss], optimizer=opt, metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxZXa-2L_gNu"
      },
      "source": [
        "### Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kWHtl-xz101"
      },
      "source": [
        "def define_generator(latent_dim, n_attributes=40):\r\n",
        "  input_label = Input(shape=(40,))\r\n",
        "\r\n",
        "  init = RandomNormal(stddev=0.02)\r\n",
        "  \r\n",
        "  image_input = Input(shape=(latent_dim,))\r\n",
        "  \r\n",
        "  merged_input = Concatenate()([image_input, input_label])  \r\n",
        "  \r\n",
        "  hid = Dense(140 * 8 * 8, activation='relu')(merged_input)    \r\n",
        "  hid = BatchNormalization(momentum=0.8)(hid)\r\n",
        "  hid = LeakyReLU(alpha=0.2)(hid)\r\n",
        "  hid = Reshape((8, 8, 140))(hid)\r\n",
        "  \r\n",
        "  # upsample to 16x16\r\n",
        "  conv = Conv2DTranspose(256, (5,5), strides=(2,2), padding='same', kernel_initializer=init)(hid)\r\n",
        "  conv = BatchNormalization(momentum=0.8)(conv)\r\n",
        "  conv = LeakyReLU(alpha=0.2)(conv)\r\n",
        "  \r\n",
        "  # upsample to 32x32\r\n",
        "  conv = Conv2DTranspose(128, (5,5), strides=(2,2), padding='same', kernel_initializer=init)(conv)\r\n",
        "  conv = BatchNormalization(momentum=0.8)(conv)\r\n",
        "  conv = LeakyReLU(alpha=0.2)(conv)\r\n",
        "  \r\n",
        "  # upsample to 64x64\r\n",
        "  conv = Conv2DTranspose(64, (5,5), strides=(2,2), padding='same', kernel_initializer=init)(conv)\r\n",
        "  conv = BatchNormalization(momentum=0.8)(conv)\r\n",
        "  conv = LeakyReLU(alpha=0.2)(conv)\r\n",
        "  \r\n",
        "  # generate\r\n",
        "  act = Conv2D(3, (16,16), activation='tanh', padding='same', kernel_initializer=init)(conv)\r\n",
        "  \r\n",
        "  model = Model([image_input, input_label], act)\r\n",
        "  \r\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhxx93X-_icW"
      },
      "source": [
        "### GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZfc-J_mz1lu"
      },
      "source": [
        "# define the combined generator and discriminator model, for updating the generator\r\n",
        "def define_gan(generator, discriminator):\r\n",
        "\t\r\n",
        "  # make weights in the discriminator not trainable\r\n",
        "  for layer in discriminator.layers:\r\n",
        "    if not isinstance(layer, BatchNormalization):\r\n",
        "      layer.trainable = False\r\n",
        "\r\n",
        "  gen_image, gen_label = generator.input\r\n",
        "\r\n",
        "  gen_output = generator.output\r\n",
        "  gan_output = discriminator(gen_output)\r\n",
        "\r\n",
        "  model = Model([gen_image, gen_label], gan_output)\r\n",
        "  \r\n",
        "\t# compile model\r\n",
        "  opt = RMSprop(lr=0.00005)\r\n",
        "  model.compile(loss=[wasserstein_loss, binary_focal_loss], optimizer=opt, metrics=['accuracy'])\r\n",
        "  return model"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hudjm-fW2KhC"
      },
      "source": [
        "## Train function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMsl4IZg2UI1",
        "outputId": "376d5900-c8ee-4865-b9bd-fa53f317bc67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# train the generator and discriminator\n",
        "def train(g_model, d_model, gan_model, inception_model, dataset, latent_dim, n_epochs=20, n_batch=64):\n",
        "    bat_per_epo = int(202599 / n_batch)\n",
        "    # manually enumerate epochs\n",
        "    for i in range(n_epochs):\n",
        "      start = time.time()\n",
        "\n",
        "      # enumerate batches over the training set\n",
        "      for j in range(bat_per_epo):\n",
        "\n",
        "        # train discriminator 3 times as the generator\n",
        "        for k in range(3):\n",
        "          # get randomly selected 'real' samples\n",
        "          [X_real, labels_real], y_real = generate_real_samples(dataset)\n",
        "          current_batch_size = X_real.shape[0]\n",
        "          \n",
        "          labels_real = np.array(labels_real).reshape(current_batch_size, 40)\n",
        "          # update discriminator model weights\n",
        "          d1_history = d_model.train_on_batch(X_real, [y_real, labels_real])\n",
        "                    \n",
        "          # generate 'fake' examples\n",
        "          [X_fake, labels_fake], y_fake = generate_fake_samples(\n",
        "              g_model, latent_dim, current_batch_size)\n",
        "          \n",
        "          # update discriminator model weights\n",
        "          d2_history = d_model.train_on_batch(X_fake, [y_fake, labels_fake])\n",
        "          \n",
        "        # prepare points in latent space as input for the generator\n",
        "        X_gan, label_gan = generate_latent_points(latent_dim, current_batch_size)\n",
        "        # create inverted labels for the fake samples\n",
        "        y_gan = -np.ones((current_batch_size, 1))\n",
        "        \n",
        "        # update the generator via the discriminator's error\n",
        "        g_history = gan_model.train_on_batch([X_gan, label_gan], [y_gan, label_gan])\n",
        "        # summarize loss on this batch\n",
        "        if ((j + 1) % 200 == 0):\n",
        "          print('Epoch %d\\tbatch %d/%d\\td1=%.3f\\td2=%.3f\\tg=%.3f' %\n",
        "              (i+1, j+1, bat_per_epo, d1_history[0], d2_history[0], g_history[0]))\n",
        "\n",
        "        if ((j + 1) % 1000 == 0):\n",
        "            \n",
        "            random_idx = np.random.randint(0, 202599)\n",
        "            custom_label = attributes['attributes'][random_idx]\n",
        "            \n",
        "            # choose a label with at least 8 attributes\n",
        "            while(len(custom_label[custom_label == 1]) < 8):\n",
        "              random_idx = np.random.randint(0, 202599)\n",
        "              custom_label = attributes['attributes'][random_idx]\n",
        "            \n",
        "            [fake, label_fake], _ = generate_fake_samples(generator, latent_dim, 64, custom_label=custom_label)\n",
        "            \n",
        "            print('\\n', decode_label(list(custom_label)))\n",
        "            save_generated_images(fake, i, j, n_batch)\n",
        "      \n",
        "      end = time.time()\n",
        "      print(\"Elapsed time: {} minutes and {:.2f} seconds\".format(int((end-start)//60), (end-start)%60))\n",
        "    \n",
        "      # save the models\n",
        "      g_model.save_weights(g_model_path_drive)\n",
        "      d_model.save_weights(d_model_path_drive)\n",
        " \n",
        "# size of the latent space\n",
        "latent_dim = 100\n",
        "\n",
        "discriminator = define_discriminator()\n",
        "generator = define_generator(latent_dim)\n",
        "gan_model = define_gan(generator, discriminator)\n",
        "\n",
        "load_weights = False\n",
        "\n",
        "if load_weights:\n",
        "\n",
        "  print(\"Resuming training...\", end=\" \")\n",
        "  discriminator.load_weights(d_model_path_drive)\n",
        "  generator.load_weights(g_model_path_drive)\n",
        "  print(\"Weights loaded\")\n",
        "\n",
        "# load image data\n",
        "dataset = load_dataset(cropped_path, attributes, 64, (64,64,3))\n",
        "\n",
        "# prepare the inception model\n",
        "inception = InceptionV3(include_top=False, pooling='avg', input_shape=(299,299,3))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 202599 validated image filenames.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZXgVnjrUQGy"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCDLh7_jUKDj",
        "outputId": "0030d976-7594-4982-c774-5c0f31d8883a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# train model\r\n",
        "train(generator, discriminator, gan_model, inception, dataset, latent_dim, n_epochs=15, n_batch=64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1\tbatch 200/3165\td1=-151.265\td2=-137.506\tg=143.882\n",
            "Epoch 1\tbatch 400/3165\td1=-276.327\td2=-262.765\tg=269.316\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXRaruOX4Tr3"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbsiiAG54XdZ"
      },
      "source": [
        "## FID value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUHYb13iU4-w",
        "outputId": "7844dc9e-07ee-4af6-81e3-2694457e97b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "real_for_fid = []\n",
        "fake_for_fid = []\n",
        "\n",
        "IMAGES_FOR_FID = 10000\n",
        "\n",
        "while len(real_for_fid) < IMAGES_FOR_FID:\n",
        "  [real, label_real], _ = generate_real_samples(dataset)\n",
        "  [fake, label_fake], _ = generate_fake_samples(generator, latent_dim, 64)\n",
        "  \n",
        "  for image in real:\n",
        "    real_for_fid.append(image)\n",
        "  for image in fake:\n",
        "    fake_for_fid.append(image)\n",
        "  \n",
        "  if len(real_for_fid) > IMAGES_FOR_FID:\n",
        "    real_for_fid = real_for_fid[:IMAGES_FOR_FID]\n",
        "    fake_for_fid = fake_for_fid[:IMAGES_FOR_FID]\n",
        "\n",
        "  print(\"\\r{}/{}\".format(len(real_for_fid), 10000), end=\"\")\n",
        "\n",
        "real_images = np.array(real_for_fid)\n",
        "fake_images = np.array(fake_for_fid)\n",
        "fid = calculate_fid(inception, real, fake)\n",
        "\n",
        "print(\"\\n\\nFid score calculated on {} real images and {} fake images: {:.2f}\".format(real_images.shape[0], fake_images.shape[0], fid))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000\n",
            "\n",
            "Fid score calculated on 10000 real images and 10000 fake images: 173.73\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZxcOjtqXXbv"
      },
      "source": [
        "## Custom label generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cML03IeHOfh9"
      },
      "source": [
        ".         |           |    |           |    |           |    |         .\r\n",
        "----------|-----------|----|-----------|----|-----------|----|----------\r\n",
        "5_o_Clock_Shadow | Blurry | Male | Sideburns\r\n",
        "Arched_Eyebrows | Brown_Hair | Mouth_Slightly_Open | Smiling\r\n",
        "Attractive | Bushy_Eyebrows | Mustache | Straight_Hair\r\n",
        "Bags_Under_Eyes | Chubby | Narrow_Eyes | Wavy_Hair\r\n",
        "Bald | Double_Chin | No_Beard | Wearing_Earrings\r\n",
        "Bangs | Eyeglasses | Oval_Face | Wearing_Hat\r\n",
        "Big_Lips | Goatee | Pale_Skin | Wearing_Lipstick\r\n",
        "Big_Nose | Gray_Hair | Pointy_Nose | Wearing_Necklace\r\n",
        "Black_Hair | Heavy_Makeup | Receding_Hairline | Wearing_Necktie\r\n",
        "Blond_Hair | High_Cheekbones | Rosy_Cheeks | Young"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBUsyjxTVbEC"
      },
      "source": [
        "[fake, label_fake], _ = generate_fake_samples(generator, latent_dim, 64)\r\n",
        "print(\"Different attributes per face\")\r\n",
        "save_generated_images(fake, 0, 0, 64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R15EdrJiXW31"
      },
      "source": [
        "print(\"Custom attributes\")\r\n",
        "for i in range(10):\r\n",
        "  random_idx = np.random.randint(0, 202599)\r\n",
        "  custom_label = attributes['attributes'][random_idx]\r\n",
        "            \r\n",
        "  # choose a label with at least 8 attributes\r\n",
        "  while(len(custom_label[custom_label == 1]) < 8):\r\n",
        "    random_idx = np.random.randint(0, 202599)\r\n",
        "    custom_label = attributes['attributes'][random_idx]  print(decode_label(custom_label))\r\n",
        "  \r\n",
        "  [real, label_real], _ = generate_real_samples(dataset)\r\n",
        "  [fake, label_fake], _ = generate_fake_samples(generator, latent_dim, 64, custom_label=custom_label)\r\n",
        "  save_generated_images(fake, 0, 0, 64)\r\n",
        "  print(\"\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}